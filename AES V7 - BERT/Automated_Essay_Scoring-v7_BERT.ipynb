{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f975f7b",
   "metadata": {},
   "source": [
    "# DATA PREPOCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "torch.cuda.set_device(1)\n",
    "device = \"cuda:%s\" % torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91a305",
   "metadata": {},
   "source": [
    "LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset .tsv\n",
    "kaggle_dataset  = pd.read_csv('./training_set_rel3.tsv', sep='\\t', encoding = \"ISO-8859-1\")\n",
    "kaggle_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d4450",
   "metadata": {},
   "source": [
    "DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Function\n",
    "def clean_dataset(input_dataset):\n",
    "  # Remove unused column \n",
    "  dataset = pd.DataFrame(\n",
    "    {\n",
    "      'essay_id' : input_dataset['essay_id'],\n",
    "      'essay_set' : input_dataset['essay_set'],\n",
    "      'essay' : input_dataset['essay'],\n",
    "      'score' : input_dataset['domain1_score']\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # Check missing value\n",
    "  missing_values = dataset.isnull().sum()\n",
    "  print(\"Jumlah missing values:\")\n",
    "  print(missing_values)\n",
    "\n",
    "  # Remove missing value\n",
    "  dataset_cleaned = dataset.dropna()\n",
    "  cleaned_missing_values = dataset_cleaned.isnull().sum()\n",
    "  print(\"\\nJumlah missing values setelah data dibersihkan:\")\n",
    "  print(cleaned_missing_values)\n",
    "\n",
    "  print(\"\\nDataset setelah kolom yang tidak dibutuhkan dan nilai kosong dihapus:\")\n",
    "\n",
    "  return dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = clean_dataset(kaggle_dataset)\n",
    "dataset_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa1543",
   "metadata": {},
   "source": [
    "SCORE NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rentang nilai esai (nilai minimum dan maksimum pada tiap set esai)\n",
    "min_max_ranges = {\n",
    "    1: (2, 12),\n",
    "    2: (1, 6),\n",
    "    3: (0, 3),\n",
    "    4: (0, 3),\n",
    "    5: (0, 4),\n",
    "    6: (0, 4),\n",
    "    7: (0, 30),\n",
    "    8: (0, 60)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score Normalization Function\n",
    "def normalize_score(dataset, min_max_ranges):\n",
    "\n",
    "    #Rumus min max normalization\n",
    "    def min_max_normalize(score, min_score, max_score):\n",
    "        return (score - min_score) / (max_score - min_score)\n",
    "    \n",
    "    #Normalisasi nilai skor\n",
    "    for essay_set, (min_score, max_score) in min_max_ranges.items():\n",
    "\n",
    "        # Filter dataset berdasarkan essay set\n",
    "        subset = dataset[dataset['essay_set'] == essay_set]\n",
    "        \n",
    "        # Lakukan normalisasi skor secara manual\n",
    "        normalized_scores = subset['score'].apply(lambda x: min_max_normalize(x, min_score, max_score))\n",
    "        \n",
    "        # Update kolom skor pada subset dataset dengan skor yang telah dinormalisasi\n",
    "        dataset.loc[subset.index, 'normalized_score'] = normalized_scores\n",
    "\n",
    "    # Ganti nilai kolom score dengan normalized_score\n",
    "    dataset['score'] = dataset['normalized_score']\n",
    "\n",
    "    # Hapus kolom normalized_score\n",
    "    dataset.drop('normalized_score', axis=1, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9730276",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized = normalize_score(dataset_cleaned, min_max_ranges)\n",
    "dataset_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f782cb",
   "metadata": {},
   "source": [
    "DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af44690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Splitting Function\n",
    "def data_splitting(dataset):\n",
    "    # Dictionary untuk menyimpan data latih dan data uji untuk setiap essay_set\n",
    "    train_data_perset = {}\n",
    "    test_data_perset = {}\n",
    "\n",
    "    # Mendefinisikan essay_set yang tersedia dalam dataset\n",
    "    essay_sets = dataset['essay_set'].unique()\n",
    "\n",
    "    for essay_set in essay_sets:\n",
    "        # Filter dataset berdasarkan essay_set\n",
    "        subset = dataset[dataset_cleaned['essay_set'] == essay_set]\n",
    "        \n",
    "        features = ['essay_id', 'essay_set', 'essay']\n",
    "        X = subset.loc[:, features]\n",
    "        y = subset.loc[:, ['score']]\n",
    "        \n",
    "        # Lakukan splitting menjadi data train (70%) dan data test (30%)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "        \n",
    "        # Menggabungkan X_train dan y_train menjadi dataframe data latih\n",
    "        train_data_perset[essay_set] = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "        # Menggabungkan X_test dan y_test menjadi dataframe data uji\n",
    "        test_data_perset[essay_set] = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # Menggabungkan semua data train dari setiap essay_set menjadi satu DataFrame data_train\n",
    "    train_data = pd.concat(train_data_perset.values(), ignore_index=True)\n",
    "\n",
    "    # Menggabungkan semua data test dari setiap essay_set menjadi satu DataFrame data_test\n",
    "    test_data = pd.concat(test_data_perset.values(), ignore_index=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f15443",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_splitting(dataset_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e724ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cetak data latih\n",
    "print(\"Train Data:\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cetak data uji\n",
    "print(\"Test Data:\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c311c",
   "metadata": {},
   "source": [
    "# EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6144a",
   "metadata": {},
   "source": [
    "Data Loader - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat kamus yang memetakan id ke suatu indeks\n",
    "def get_id2emb(ids):\n",
    "\n",
    "  id2emb = {}\n",
    "  for n,id in enumerate(ids.to_list()):\n",
    "    id2emb[id] = n\n",
    "\n",
    "  print('Essay ids to embeddings dictionary created.')\n",
    "  \n",
    "  return id2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2emb_train = get_id2emb(train_data['essay_id'])\n",
    "id2emb_test = get_id2emb(test_data['essay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, id2emb, essay_embeddings, batch_size, shuffle):\n",
    "    \n",
    "    # Extract embeddings for each essay_id using the id2emb dictionary\n",
    "    embeddings = np.array([essay_embeddings[id2emb[id]] for id in df['essay_id']])\n",
    "    \n",
    "    # Extract scores from the DataFrame\n",
    "    scores = np.array(df['score'])\n",
    "    \n",
    "    # Create a PyTorch TensorDataset from the embeddings and scores\n",
    "    data = TensorDataset(torch.from_numpy(embeddings).float(), torch.from_numpy(scores).float())\n",
    "    \n",
    "    # Create a PyTorch DataLoader from the TensorDataset\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe616ca",
   "metadata": {},
   "source": [
    "# EMBEDDING BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6e18b",
   "metadata": {},
   "source": [
    "LOAD PRETRAINED MODEL BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f122b2-984c-4a8b-9069-6c7f2e72ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat pretrained BERT dan tokenizer\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\").to(device)\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb9209-2506-4538-bafa-239ce7dc1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65fdc-dbab-4e59-9942-9066d3e19464",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer_bert.vocab_size\n",
    "print(f\"Jumlah kata dalam kamus tokenizer: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d2de5-c683-4cb2-8d5d-1b5a06e78a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_bert([\"waiting for my turn\"])\n",
    "print(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35cfe9-a9c5-4b63-8c54-02e0bcea412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Akses embedding layer\n",
    "embedding_layer = bert_model.embeddings\n",
    "\n",
    "# Cetak informasi embedding layer\n",
    "print(embedding_layer)\n",
    "print(embedding_layer.word_embeddings.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85faaa9c-fdff-45e3-89d9-510719ae0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer_bert(['waiting']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6749b-b3c1-4100-b49a-878346bc888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.word_embeddings.weight[2613])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c366858",
   "metadata": {},
   "source": [
    "WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11092f54-e05a-4088-95cc-35127ff54524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding(essay_list, tokenizer, model):\n",
    "\n",
    "  print('Encoding essay embeddings:')\n",
    "\n",
    "  embeddings = []\n",
    "  for essay in tqdm(essay_list):\n",
    "    encoded_input = tokenizer(essay, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "      model_output = model(**encoded_input)\n",
    "    tokens_embeddings = np.matrix(model_output[0].squeeze().cpu())\n",
    "    embeddings.append(np.squeeze(np.asarray(tokens_embeddings.mean(0))))\n",
    "\n",
    "  return np.matrix(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3304de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan Embeddings yang dihasilkan BERT \n",
    "train_embeddings_bert = bert_embedding(train_data['essay'], tokenizer_bert, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0096a-4465-4d39-8a9a-b5b052d59e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_bert = bert_embedding(test_data['essay'], tokenizer_bert, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb41cf-ddc7-4b7a-afd0-ab5428fe29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf55bf4-d57f-43ad-9aed-a788bfcfabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(train_embeddings_bert)\n",
    "train_embeddings_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60586eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_bert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29094334",
   "metadata": {},
   "source": [
    "# REGRESI FCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f995c30",
   "metadata": {},
   "source": [
    "INISIALISASI FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3cd82-b670-4994-83b1-08c79be09469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menginisialisasi FCNN\n",
    "class FCNN(nn.Module):\n",
    "    # Fungsi untuk menentukan pengaturan layer\n",
    "    def __init__(self, input_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 397) \n",
    "        self.dropout1 = nn.Dropout(0.3) \n",
    "        self.fc2 = nn.Linear(397, 32)          \n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(32, 1)              \n",
    "        self.sigmoid = nn.Sigmoid()             \n",
    "    \n",
    "    # Fungsi untuk untuk melakukan feedforward\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))             \n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))              \n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc3(x)                                    \n",
    "        return self.sigmoid(x)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11c3e1",
   "metadata": {},
   "source": [
    "TESTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(trained_model, cost_function, test_loader):\n",
    "    trained_model.eval() # Mengatur model ke mode evaluasi (eval mode)\n",
    "    test_loss = 0.\n",
    "    samples = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, targets) in enumerate(test_loader):\n",
    "            \n",
    "            # Menghapus dimensi yang tidak perlu dari inputs dan mentransfer ke device\n",
    "            inputs = inputs.squeeze(dim=1).to(device)\n",
    "            \n",
    "            # Menyesuaikan dimensi targets dan mentransfer ke device\n",
    "            targets = targets.reshape(targets.shape[0], 1).to(device)\n",
    "            \n",
    "            # Menghitung output model (prediksi) dari inputs\n",
    "            outputs = trained_model(inputs).reshape(-1, 1)\n",
    "            \n",
    "            # Menghitung nilai loss dengan membandingkan outputs dengan targets\n",
    "            loss = cost_function(outputs, targets)\n",
    "            \n",
    "            # Menghitung jumlah sampel dalam batch\n",
    "            samples += inputs.shape[0]\n",
    "            \n",
    "            # Menambahkan nilai loss dari batch ke test_loss\n",
    "            test_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "    # Menghitung rata-rata loss di seluruh batch (samples)\n",
    "    avg_loss = test_loss / samples\n",
    "    \n",
    "    # Mengembalikan nilai rata-rata loss\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72e3c7",
   "metadata": {},
   "source": [
    "TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh fungsi untuk training model\n",
    "def training_step(model, cost_function, train_loader, test_loader, save_path, num_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Mengatur model ke mode pelatihan\n",
    "        \n",
    "        # Mengatur gradien parameter ke nilai nol untuk iterasi\n",
    "        running_loss = 0.\n",
    "        samples = 0.\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            # Menghapus dimensi yang tidak perlu dari inputs dan mentransfer ke device\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "            # Menyesuaikan dimensi targets dan mentransfer ke device\n",
    "            targets = targets.reshape(targets.shape[0], 1).to(device)\n",
    "            \n",
    "            # Menghitung output model (prediksi) dari inputs\n",
    "            outputs = model(inputs).reshape(-1, 1)\n",
    "            \n",
    "            # Menghitung nilai loss dengan membandingkan outputs dengan targets\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Mengatur gradien parameter ke nilai nol untuk iterasi berikutnya\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Melakukan backpropagation untuk menghitung gradien loss terhadap parameter model\n",
    "            loss.backward()\n",
    "            \n",
    "            # Melakukan optimizer untuk mengupdate parameter model berdasarkan gradien\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Menambahkan nilai loss dari batch ke running_loss\n",
    "            running_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "            # Menghitung jumlah sampel dalam batch\n",
    "            samples += inputs.shape[0]\n",
    "        \n",
    "        # Menghitung rata-rata loss pada data latih\n",
    "        train_loss = running_loss / samples\n",
    "        \n",
    "         # Evaluasi pada data uji\n",
    "        test_loss = test_step(model, cost_function, test_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print('Epoch: {:}/{:}\\tLoss/train: {:.5f}\\tLoss/test: {:.5f}'.format(epoch+1, num_epochs, train_loss, test_loss))\n",
    "    \n",
    "    # Simpan model setelah pelatihan\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved at {save_path}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44aae7",
   "metadata": {},
   "source": [
    "SCORING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi untuk melakukan prediksi pada data uji\n",
    "def scoring(trained_model, test_loader):\n",
    "    trained_model.to(device)  # Move the model to the correct device\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # Lakukan prediksi dengan model yang telah dilatih\n",
    "            outputs = trained_model(inputs)\n",
    "            \n",
    "            # Menyimpan prediksi (outputs) dalam bentuk list predictions\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf218006",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da919b-309a-4e92-ae31-e89cfbe0cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e9831-1d61-4cd1-a27d-1092768abd13",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c309f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "input_size = 768\n",
    "batch_size = \n",
    "epochs = \n",
    "lr = \n",
    "\n",
    "# TRAINING\n",
    "# Inisialisasi model, loader, dan fungsi loss\n",
    "model_bert = FCNN(input_size).to(device)  # Ganti dengan model Anda\n",
    "cost_function = torch.nn.MSELoss()\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_bert = get_loader(train_data, id2emb_train, train_embeddings_bert, batch_size, shuffle=True)\n",
    "test_loader_bert = get_loader(test_data, id2emb_test, test_embeddings_bert, batch_size, shuffle=False)\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "print(f\"\\t\\t\\tTraining model BERT: \")\n",
    "print('------------------------------------------------------------------')\n",
    "# Path tempat model akan disimpan dan dimuat\n",
    "save_path = 'model_bert_v7-7.9.5.pth'\n",
    "\n",
    "start_time_bert = time.time()\n",
    "train_loss_bert, test_loss_bert = training_step(model_bert, cost_function, train_loader_bert, test_loader_bert, save_path, epochs, lr)\n",
    "end_time_bert = time.time()\n",
    "\n",
    "print('Training time:', end_time_bert - start_time_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d2851-6ab7-43a9-9129-b6088f631130",
   "metadata": {},
   "source": [
    "SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608de217-f7e9-444e-a014-ed3a94ba22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat model yang telah dilatih\n",
    "model_bert_trained = model_bert \n",
    "model_bert_trained.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Menggunakan model untuk prediksi pada data uji\n",
    "print('------------------------------------------------------------------')\n",
    "print(f\"\\t\\t\\tScoring Essay: \")\n",
    "print('------------------------------------------------------------------')\n",
    "start_time_eval = time.time()\n",
    "test_predictions_bert = scoring(model_bert_trained, test_loader_bert)\n",
    "end_time_eval = time.time()\n",
    "print('Evaluation time:', end_time_eval - start_time_eval)\n",
    "\n",
    "# store train_df, test_df and predictions\n",
    "train_df_bert = train_data\n",
    "test_df_bert = test_data\n",
    "preds_bert = test_predictions_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa091b",
   "metadata": {},
   "source": [
    "# RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_df(test_df, model_preds):\n",
    "\n",
    "  # create new results df with model scaled preds\n",
    "  preds_df = pd.DataFrame(model_preds)\n",
    "  results_df = test_df.reset_index(drop=True)\\\n",
    "              .join(preds_df)\\\n",
    "              .rename(columns={0:'prediction'})\\\n",
    "              .sort_values(by='essay_id')\\\n",
    "              .reset_index(drop=True)\n",
    "\n",
    "  # move score to last colum\n",
    "  s_df = results_df.pop('score')\n",
    "  results_df['score'] = s_df\n",
    "\n",
    "  return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87715126",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_df(test_df_bert, preds_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce84069",
   "metadata": {},
   "source": [
    "# DENORMALISASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize_score(score, min_max_range):\n",
    "    # Mendapatkan nilai minimum dan maksimum dari rentang normalisasi\n",
    "    min_score, max_score = min_max_range\n",
    "    \n",
    "    # Mengembalikan skor esai yang sudah dinormalisasi ke rentang aslinya\n",
    "    return round(score * (max_score - min_score) + min_score)\n",
    "\n",
    "def restore_original_scores(df, preds, min_max_ranges):\n",
    "    # Membuat salinan dataframe untuk menghindari modifikasi dataframe asli\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Mendapatkan kolom skor aktual\n",
    "    actual_scores = df_copy['score'].values\n",
    "    \n",
    "    # Mendapatkan kolom essay_set\n",
    "    essay_sets = df_copy['essay_set'].values\n",
    "    \n",
    "    # Memastikan preds memiliki panjang yang sama dengan jumlah data\n",
    "    assert len(preds) == len(df_copy), \"Length of predictions does not match length of dataframe\"\n",
    "    \n",
    "    # Memulihkan skor prediksi dan skor aktual ke rentang aslinya\n",
    "    restored_preds = [inverse_normalize_score(pred, min_max_ranges[essay_set]) for pred, essay_set in zip(preds, essay_sets)]\n",
    "    restored_actuals = [inverse_normalize_score(actual, min_max_ranges[essay_set]) for actual, essay_set in zip(actual_scores, essay_sets)]\n",
    "    \n",
    "    # Mengganti kolom skor prediksi dan aktual dengan skor yang sudah dipulihkan\n",
    "    df_copy['prediction'] = restored_preds\n",
    "    df_copy['score'] = restored_actuals\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengembalikan skor prediksi dan skor aktual ke rentang awalnya\n",
    "restored_results_df_bert = restore_original_scores(test_df_bert, preds_bert, min_max_ranges)\n",
    "\n",
    "# Cetak hasilnya\n",
    "print(\"Restored Results:\")\n",
    "restored_results_df_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144dadc",
   "metadata": {},
   "source": [
    "# EVALUASI QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640c52c-d4a5-4cd8-9f09-1231b01e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_qwk(actuals, preds):\n",
    "    # Menentukan nilai minimum dan maksimum untuk rentang skor\n",
    "    min_rating = min(min(actuals), min(preds))\n",
    "    max_rating = max(max(actuals), max(preds))\n",
    "    \n",
    "    # Jumlah total kemungkinan penilaian\n",
    "    num_ratings = max_rating - min_rating + 1\n",
    "\n",
    "    # Membuat matriks bobot W\n",
    "    weight_mat = np.zeros((num_ratings, num_ratings))\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            weight_mat[i][j] = ((i - j) ** 2) / ((num_ratings - 1) ** 2)\n",
    "\n",
    "    # Membuat matriks observasi O\n",
    "    conf_mat = np.zeros((num_ratings, num_ratings))\n",
    "    for actual, pred in zip(actuals, preds):\n",
    "        conf_mat[actual - min_rating][pred - min_rating] += 1\n",
    "\n",
    "    # Membuat matriks ekspektasi E\n",
    "    actual_hist = np.zeros(num_ratings)\n",
    "    pred_hist = np.zeros(num_ratings)\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            actual_hist[i] += conf_mat[i][j]\n",
    "            pred_hist[j] += conf_mat[i][j]\n",
    "\n",
    "    expected_mat = np.outer(actual_hist, pred_hist) / len(actuals)\n",
    "\n",
    "    # Menghitung nilai QWK\n",
    "    num_agreements = np.sum(weight_mat * conf_mat)\n",
    "    num_possible_agreements = np.sum(weight_mat * expected_mat)\n",
    "    kappa_score = 1 - (num_agreements / num_possible_agreements)\n",
    "\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443356b-4b36-4618-b361-924b045088a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung QWK per set\n",
    "def calculate_qwk_per_set(df):\n",
    "    # Menyimpan nilai QWK per set dalam dictionary\n",
    "    qwk_per_set = {}\n",
    "    \n",
    "    # Mendapatkan unique essay_set values\n",
    "    essay_sets = df['essay_set'].unique()\n",
    "    \n",
    "    # Iterasi melalui setiap essay_set\n",
    "    for essay_set in essay_sets:\n",
    "        # Filter dataframe berdasarkan essay_set\n",
    "        subset_df = df[df['essay_set'] == essay_set]\n",
    "        \n",
    "        # Mengekstrak skor aktual dan prediksi dari subset dataframe\n",
    "        actual_scores = subset_df['score'].astype(int)\n",
    "        predicted_scores = subset_df['prediction'].astype(int)\n",
    "        \n",
    "        # Menghitung QWK untuk subset tersebut\n",
    "        qwk = calculate_qwk(actual_scores, predicted_scores)\n",
    "        \n",
    "        # Menyimpan nilai QWK ke dalam dictionary\n",
    "        qwk_per_set[f'Set {essay_set}'] = qwk\n",
    "    \n",
    "    return qwk_per_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d8cda-6896-4180-a9a3-fa9fc45647ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung QWK per set\n",
    "qwk_per_set_bert = calculate_qwk_per_set(restored_results_df_bert)\n",
    "\n",
    "# Menampilkan nilai QWK per set\n",
    "for essay_set, qwk_score in qwk_per_set_bert.items():\n",
    "    print(f\"Quadratic Weighted Kappa Score for {essay_set}: {qwk_score}\")\n",
    "\n",
    "# Menghitung rata-rata nilai QWK dari semua set\n",
    "average_qwk_bert = np.mean(list(qwk_per_set_bert.values()))\n",
    "\n",
    "# Menampilkan rata-rata nilai QWK\n",
    "print(\"Average Quadratic Weighted Kappa Score:\", average_qwk_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
