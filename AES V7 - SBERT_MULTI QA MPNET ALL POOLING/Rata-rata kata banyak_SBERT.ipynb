{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f975f7b",
   "metadata": {},
   "source": [
    "# DATA PREPOCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3a66f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b2a1bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "torch.cuda.set_device(1)\n",
    "device = \"cuda:%s\" % torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91a305",
   "metadata": {},
   "source": [
    "LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61ee9377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                   4               4             NaN              8   \n",
       "1                   5               4             NaN              9   \n",
       "2                   4               3             NaN              7   \n",
       "3                   5               5             NaN             10   \n",
       "4                   4               4             NaN              8   \n",
       "...               ...             ...             ...            ...   \n",
       "12971              17              18             NaN             35   \n",
       "12972              15              17             NaN             32   \n",
       "12973              20              26            40.0             40   \n",
       "12974              20              20             NaN             40   \n",
       "12975              20              20             NaN             40   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0                 NaN             NaN            NaN  ...            NaN   \n",
       "1                 NaN             NaN            NaN  ...            NaN   \n",
       "2                 NaN             NaN            NaN  ...            NaN   \n",
       "3                 NaN             NaN            NaN  ...            NaN   \n",
       "4                 NaN             NaN            NaN  ...            NaN   \n",
       "...               ...             ...            ...  ...            ...   \n",
       "12971             NaN             NaN            NaN  ...            4.0   \n",
       "12972             NaN             NaN            NaN  ...            4.0   \n",
       "12973             NaN             NaN            NaN  ...            5.0   \n",
       "12974             NaN             NaN            NaN  ...            4.0   \n",
       "12975             NaN             NaN            NaN  ...            4.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12971            4.0            4.0            3.0            NaN   \n",
       "12972            4.0            4.0            3.0            NaN   \n",
       "12973            5.0            5.0            5.0            4.0   \n",
       "12974            4.0            4.0            4.0            NaN   \n",
       "12975            4.0            4.0            4.0            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12971            NaN            NaN            NaN            NaN   \n",
       "12972            NaN            NaN            NaN            NaN   \n",
       "12973            4.0            4.0            4.0            4.0   \n",
       "12974            NaN            NaN            NaN            NaN   \n",
       "12975            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "12971            NaN  \n",
       "12972            NaN  \n",
       "12973            4.0  \n",
       "12974            NaN  \n",
       "12975            NaN  \n",
       "\n",
       "[12976 rows x 28 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset .tsv\n",
    "kaggle_dataset  = pd.read_csv('./training_set_rel3.tsv', sep='\\t', encoding = \"ISO-8859-1\")\n",
    "kaggle_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d4450",
   "metadata": {},
   "source": [
    "DATA CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5cc2b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Function\n",
    "def clean_dataset(input_dataset):\n",
    "  # Remove unused column \n",
    "  dataset = pd.DataFrame(\n",
    "    {\n",
    "      'essay_id' : input_dataset['essay_id'],\n",
    "      'essay_set' : input_dataset['essay_set'],\n",
    "      'essay' : input_dataset['essay'],\n",
    "      'score' : input_dataset['domain1_score']\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # Check missing value\n",
    "  missing_values = dataset.isnull().sum()\n",
    "  print(\"Jumlah missing values:\")\n",
    "  print(missing_values)\n",
    "\n",
    "  # Remove missing value\n",
    "  dataset_cleaned = dataset.dropna()\n",
    "  cleaned_missing_values = dataset_cleaned.isnull().sum()\n",
    "  print(\"\\nJumlah missing values setelah data dibersihkan:\")\n",
    "  print(cleaned_missing_values)\n",
    "\n",
    "  print(\"\\nDataset setelah kolom yang tidak dibutuhkan dan nilai kosong dihapus:\")\n",
    "\n",
    "  return dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9858a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah missing values:\n",
      "essay_id     0\n",
      "essay_set    0\n",
      "essay        0\n",
      "score        0\n",
      "dtype: int64\n",
      "\n",
      "Jumlah missing values setelah data dibersihkan:\n",
      "essay_id     0\n",
      "essay_set    0\n",
      "essay        0\n",
      "score        0\n",
      "dtype: int64\n",
      "\n",
      "Dataset setelah kolom yang tidak dibutuhkan dan nilai kosong dihapus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       score  \n",
       "0          8  \n",
       "1          9  \n",
       "2          7  \n",
       "3         10  \n",
       "4          8  \n",
       "...      ...  \n",
       "12971     35  \n",
       "12972     32  \n",
       "12973     40  \n",
       "12974     40  \n",
       "12975     40  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned = clean_dataset(kaggle_dataset)\n",
    "dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae920344-b39c-4e9b-9e71-72f675e7c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Average Word Counts per Essay Set:\n",
      "essay_set\n",
      "1    375\n",
      "2    373\n",
      "3    375\n",
      "4    357\n",
      "6    358\n",
      "7    370\n",
      "8    374\n",
      "Name: essay, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rata-rata jumlah kata pada tiap set esai\n",
    "avg_word_counts = dataset_cleaned.groupby('essay_set')['essay'].apply(lambda x: x.str.split().str.len().mean()).round().astype(int)\n",
    "\n",
    "# Filter data berdasarkan rata-rata jumlah kata yang diinginkan\n",
    "target_avg_word_count = 374\n",
    "filtered_dataset = pd.DataFrame(columns=dataset_cleaned.columns)\n",
    "for essay_set, avg_count in avg_word_counts.items():\n",
    "    subset = dataset_cleaned[dataset_cleaned['essay_set'] == essay_set]\n",
    "    filtered_subset = subset[(subset['essay'].str.split().str.len() >= target_avg_word_count - 20) & \n",
    "                             (subset['essay'].str.split().str.len() <= target_avg_word_count + 20)]\n",
    "    filtered_dataset = pd.concat([filtered_dataset, filtered_subset])\n",
    "\n",
    "# Periksa distribusi jumlah kata setelah filtering\n",
    "filtered_avg_word_counts = filtered_dataset.groupby('essay_set')['essay'].apply(lambda x: x.str.split().str.len().mean()).round().astype(int)\n",
    "print(\"Filtered Average Word Counts per Essay Set:\")\n",
    "print(filtered_avg_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e63bffc0-6e77-46ab-9f67-d4c2c2555687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Data per Set Esai:\n",
      "essay_set\n",
      "1    234\n",
      "2    200\n",
      "3      1\n",
      "4      1\n",
      "6      2\n",
      "7     33\n",
      "8     33\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah data pada setiap set esai\n",
    "jumlah_data_per_set = filtered_dataset.groupby('essay_set').size()\n",
    "\n",
    "# Menampilkan jumlah data pada setiap set esai\n",
    "print(\"Jumlah Data per Set Esai:\")\n",
    "print(jumlah_data_per_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0935446b-a2af-4c2a-9a15-bfba3c1dfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_to_remove = [3, 4, 6]\n",
    "# Filter out the specified sets\n",
    "filtered_dataset = filtered_dataset[~filtered_dataset['essay_set'].isin(sets_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "574c403d-7b38-44d1-907f-8c3a72340f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Data per Set Esai:\n",
      "essay_set\n",
      "1    234\n",
      "2    200\n",
      "7     33\n",
      "8     33\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah data pada setiap set esai\n",
    "jumlah_data_per_set = filtered_dataset.groupby('essay_set').size()\n",
    "\n",
    "# Menampilkan jumlah data pada setiap set esai\n",
    "print(\"Jumlah Data per Set Esai:\")\n",
    "print(jumlah_data_per_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa1543",
   "metadata": {},
   "source": [
    "SCORE NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4d7572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rentang nilai esai (nilai minimum dan maksimum pada tiap set esai)\n",
    "min_max_ranges = {\n",
    "    1: (2, 12),\n",
    "    2: (1, 6),\n",
    "    3: (0, 3),\n",
    "    4: (0, 3),\n",
    "    5: (0, 4),\n",
    "    6: (0, 4),\n",
    "    7: (0, 30),\n",
    "    8: (0, 60)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8295079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score Normalization Function\n",
    "def normalize_score(dataset, min_max_ranges):\n",
    "\n",
    "    #Rumus min max normalization\n",
    "    def min_max_normalize(score, min_score, max_score):\n",
    "        return (score - min_score) / (max_score - min_score)\n",
    "    \n",
    "    #Normalisasi nilai skor\n",
    "    for essay_set, (min_score, max_score) in min_max_ranges.items():\n",
    "\n",
    "        # Filter dataset berdasarkan essay set\n",
    "        subset = dataset[dataset['essay_set'] == essay_set]\n",
    "        \n",
    "        # Lakukan normalisasi skor secara manual\n",
    "        normalized_scores = subset['score'].apply(lambda x: min_max_normalize(x, min_score, max_score))\n",
    "        \n",
    "        # Update kolom skor pada subset dataset dengan skor yang telah dinormalisasi\n",
    "        dataset.loc[subset.index, 'normalized_score'] = normalized_scores\n",
    "\n",
    "    # Ganti nilai kolom score dengan normalized_score\n",
    "    dataset['score'] = dataset['normalized_score']\n",
    "\n",
    "    # Hapus kolom normalized_score\n",
    "    dataset.drop('normalized_score', axis=1, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9730276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2 I feel that computers do ta...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I must admit that the ex...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you think that computers are useless? Or do...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper, Computers are high tec and hav...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12887</th>\n",
       "      <td>21514</td>\n",
       "      <td>8</td>\n",
       "      <td>I think laughter should be a huge part in eve...</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12904</th>\n",
       "      <td>21537</td>\n",
       "      <td>8</td>\n",
       "      <td>In the @DATE1 of @NUM1' I spent two weeks at ...</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12948</th>\n",
       "      <td>21595</td>\n",
       "      <td>8</td>\n",
       "      <td>Have you ever experienced a time with your fr...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12949</th>\n",
       "      <td>21596</td>\n",
       "      <td>8</td>\n",
       "      <td>I woke up just like any other day happy yet l...</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12950</th>\n",
       "      <td>21598</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is an important part of my life, eit...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id essay_set                                              essay  \\\n",
       "11          12         1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
       "17          18         1  Dear Local Newspaper, I must admit that the ex...   \n",
       "20          21         1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
       "25          26         1  Do you think that computers are useless? Or do...   \n",
       "27          28         1  Dear Newspaper, Computers are high tec and hav...   \n",
       "...        ...       ...                                                ...   \n",
       "12887    21514         8   I think laughter should be a huge part in eve...   \n",
       "12904    21537         8   In the @DATE1 of @NUM1' I spent two weeks at ...   \n",
       "12948    21595         8   Have you ever experienced a time with your fr...   \n",
       "12949    21596         8   I woke up just like any other day happy yet l...   \n",
       "12950    21598         8   Laughter is an important part of my life, eit...   \n",
       "\n",
       "          score  \n",
       "11     0.600000  \n",
       "17     0.600000  \n",
       "20     0.600000  \n",
       "25     0.700000  \n",
       "27     0.700000  \n",
       "...         ...  \n",
       "12887  0.566667  \n",
       "12904  0.550000  \n",
       "12948  0.600000  \n",
       "12949  0.516667  \n",
       "12950  0.500000  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_normalized = normalize_score(filtered_dataset, min_max_ranges)\n",
    "dataset_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f782cb",
   "metadata": {},
   "source": [
    "DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3af44690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting(dataset):\n",
    "    # Dictionary untuk menyimpan data latih dan data uji untuk setiap essay_set\n",
    "    train_data_perset = {}\n",
    "    test_data_perset = {}\n",
    "\n",
    "    # Mendefinisikan essay_set yang tersedia dalam dataset\n",
    "    essay_sets = dataset['essay_set'].unique()\n",
    "\n",
    "    for essay_set in essay_sets:\n",
    "        # Filter dataset berdasarkan essay_set\n",
    "        subset = dataset[dataset['essay_set'] == essay_set]\n",
    "        \n",
    "        features = ['essay_id', 'essay_set', 'essay']\n",
    "        X = subset.loc[:, features]\n",
    "        y = subset.loc[:, ['score']]\n",
    "        \n",
    "        # Lakukan splitting menjadi data train (70%) dan data test (30%)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "        \n",
    "        # Menggabungkan X_train dan y_train menjadi dataframe data latih\n",
    "        train_data_perset[essay_set] = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "        # Menggabungkan X_test dan y_test menjadi dataframe data uji\n",
    "        test_data_perset[essay_set] = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # Menggabungkan semua data train dari setiap essay_set menjadi satu DataFrame data_train\n",
    "    train_data = pd.concat(train_data_perset.values(), ignore_index=True)\n",
    "\n",
    "    # Menggabungkan semua data test dari setiap essay_set menjadi satu DataFrame data_test\n",
    "    test_data = pd.concat(test_data_perset.values(), ignore_index=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5f15443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = data_splitting(dataset_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8e724ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1052</td>\n",
       "      <td>1</td>\n",
       "      <td>Did you know that @PERCENT2 out of @PERCENT1 c...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>645</td>\n",
       "      <td>1</td>\n",
       "      <td>The computer effects peoplelife by taking them...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 people throughout...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>I think the computer dire a postive effect on ...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1012</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper writer, I'm writting to t...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>21598</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is an important part of my life, eit...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>21016</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is such a great joy in my life. If p...</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>21115</td>\n",
       "      <td>8</td>\n",
       "      <td>There are many reasons why laughter is an imp...</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>21249</td>\n",
       "      <td>8</td>\n",
       "      <td>It was the first day of the ninth grade, that...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>21514</td>\n",
       "      <td>8</td>\n",
       "      <td>I think laughter should be a huge part in eve...</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    essay_id essay_set                                              essay  \\\n",
       "0       1052         1  Did you know that @PERCENT2 out of @PERCENT1 c...   \n",
       "1        645         1  The computer effects peoplelife by taking them...   \n",
       "2         29         1  Dear local newspaper, @CAPS1 people throughout...   \n",
       "3        476         1  I think the computer dire a postive effect on ...   \n",
       "4       1012         1  Dear local newspaper writer, I'm writting to t...   \n",
       "..       ...       ...                                                ...   \n",
       "344    21598         8   Laughter is an important part of my life, eit...   \n",
       "345    21016         8   Laughter is such a great joy in my life. If p...   \n",
       "346    21115         8   There are many reasons why laughter is an imp...   \n",
       "347    21249         8   It was the first day of the ninth grade, that...   \n",
       "348    21514         8   I think laughter should be a huge part in eve...   \n",
       "\n",
       "        score  \n",
       "0    0.700000  \n",
       "1    0.600000  \n",
       "2    0.700000  \n",
       "3    0.600000  \n",
       "4    0.700000  \n",
       "..        ...  \n",
       "344  0.500000  \n",
       "345  0.516667  \n",
       "346  0.533333  \n",
       "347  0.700000  \n",
       "348  0.566667  \n",
       "\n",
       "[349 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cetak data latih\n",
    "print(\"Train Data:\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85c2aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 Newspaper, @CAPS2 though computers...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1614</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 I think that computers are benefic...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1392</td>\n",
       "      <td>1</td>\n",
       "      <td>I think that computers have a good effect on p...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1055</td>\n",
       "      <td>1</td>\n",
       "      <td>To whom it @MONTH1 concern, To many people it ...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>21090</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of Laughter @CAPS2 friends and I, a...</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>21369</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is the key I think that being happy ...</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>21407</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>21204</td>\n",
       "      <td>8</td>\n",
       "      <td>It was the first day of sophomore year. I had...</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>20747</td>\n",
       "      <td>8</td>\n",
       "      <td>These days you don't really need a reason to ...</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    essay_id essay_set                                              essay  \\\n",
       "0        560         1  Dear @CAPS1 Newspaper, @CAPS2 though computers...   \n",
       "1       1614         1  Dear @CAPS1 I think that computers are benefic...   \n",
       "2       1392         1  I think that computers have a good effect on p...   \n",
       "3         73         1  Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...   \n",
       "4       1055         1  To whom it @MONTH1 concern, To many people it ...   \n",
       "..       ...       ...                                                ...   \n",
       "146    21090         8  The @CAPS1 of Laughter @CAPS2 friends and I, a...   \n",
       "147    21369         8   Laughter is the key I think that being happy ...   \n",
       "148    21407         8  The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...   \n",
       "149    21204         8   It was the first day of sophomore year. I had...   \n",
       "150    20747         8   These days you don't really need a reason to ...   \n",
       "\n",
       "        score  \n",
       "0    0.600000  \n",
       "1    0.700000  \n",
       "2    0.300000  \n",
       "3    0.600000  \n",
       "4    0.600000  \n",
       "..        ...  \n",
       "146  0.616667  \n",
       "147  0.483333  \n",
       "148  0.666667  \n",
       "149  0.516667  \n",
       "150  0.633333  \n",
       "\n",
       "[151 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cetak data uji\n",
    "print(\"Test Data:\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "861fab0c-593d-4782-a3a4-f4c693f28f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1: 71 test data\n",
      "Set 2: 60 test data\n",
      "Set 3: 0 test data\n",
      "Set 4: 0 test data\n",
      "Set 5: 0 test data\n",
      "Set 6: 0 test data\n",
      "Set 7: 10 test data\n",
      "Set 8: 10 test data\n"
     ]
    }
   ],
   "source": [
    "# Function to count test data for each essay set\n",
    "def count_test_data(test_data):\n",
    "    # Create a dictionary to store the count of test data for each essay set\n",
    "    test_data_count = {}\n",
    "\n",
    "    # Loop through each essay set (1 to 8)\n",
    "    for essay_set in range(1, 9):\n",
    "        # Filter test data for the current essay set\n",
    "        subset = test_data[test_data['essay_set'] == essay_set]\n",
    "        \n",
    "        # Count the number of rows in the subset\n",
    "        count = subset.shape[0]\n",
    "        \n",
    "        # Store the count in the dictionary\n",
    "        test_data_count[essay_set] = count\n",
    "\n",
    "    return test_data_count\n",
    "\n",
    "# Get the count of test data for each essay set\n",
    "test_data_count = count_test_data(test_data)\n",
    "\n",
    "# Print the count of test data for each essay set\n",
    "for essay_set, count in test_data_count.items():\n",
    "    print(f\"Set {essay_set}: {count} test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6144a",
   "metadata": {},
   "source": [
    "Data Loader - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82d3d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat kamus yang memetakan id ke suatu indeks\n",
    "def get_id2emb(ids):\n",
    "\n",
    "  id2emb = {}\n",
    "  for n,id in enumerate(ids.to_list()):\n",
    "    id2emb[id] = n\n",
    "\n",
    "  print('Essay ids to embeddings dictionary created.')\n",
    "  \n",
    "  return id2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50af486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay ids to embeddings dictionary created.\n",
      "Essay ids to embeddings dictionary created.\n"
     ]
    }
   ],
   "source": [
    "id2emb_train = get_id2emb(train_data['essay_id'])\n",
    "id2emb_test = get_id2emb(test_data['essay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9847c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, id2emb, essay_embeddings, batch_size, shuffle):\n",
    "    \n",
    "    # Extract embeddings for each essay_id using the id2emb dictionary\n",
    "    embeddings = np.array([essay_embeddings[id2emb[id]] for id in df['essay_id']])\n",
    "    \n",
    "    # Extract scores from the DataFrame\n",
    "    scores = np.array(df['score'])\n",
    "    \n",
    "    # Create a PyTorch TensorDataset from the embeddings and scores\n",
    "    data = TensorDataset(torch.from_numpy(embeddings).float(), torch.from_numpy(scores).float())\n",
    "    \n",
    "    # Create a PyTorch DataLoader from the TensorDataset\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe616ca",
   "metadata": {},
   "source": [
    "# EMBEDDING SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6e18b",
   "metadata": {},
   "source": [
    "LOAD PRETRAINED MODEL SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f904e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifatiad42021/miniconda3/envs/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model information:\n",
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Memuat pretrained SBERT dan tokenizer\n",
    "tokenizer_sbert = AutoTokenizer.from_pretrained('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "sbert_model = AutoModel.from_pretrained('sentence-transformers/multi-qa-mpnet-base-dot-v1').to(device)\n",
    "\n",
    "# Mencetak informasi tentang model\n",
    "print(\"Model information:\")\n",
    "print(sbert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c366858",
   "metadata": {},
   "source": [
    "SENTENCE EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf4b4d13-a041-4672-8ed8-4e63160536b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_embedding(essay_list, tokenizer, model):\n",
    "\n",
    "    print('Encoding essay embeddings:')\n",
    "\n",
    "    embeddings = []\n",
    "    for essay in tqdm(essay_list):\n",
    "        encoded_input = tokenizer(essay, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Get the token embeddings (excluding the batch dimension)\n",
    "        token_embeddings = model_output.last_hidden_state.squeeze().cpu().numpy()\n",
    "\n",
    "        # Compute mean pooling\n",
    "        mean_pooling = np.mean(token_embeddings, axis=0)\n",
    "\n",
    "        # Compute max pooling\n",
    "        max_pooling = np.max(token_embeddings, axis=0)\n",
    "\n",
    "        # Use the embedding of the CLS token (first token) for each input\n",
    "        cls_pooling = model_output.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "        # Concatenate mean and max pooling results\n",
    "        all_pooling = np.concatenate((mean_pooling, max_pooling, cls_pooling))\n",
    "        \n",
    "        embeddings.append(all_pooling)\n",
    "\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e3304de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding essay embeddings:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2427653d4e274826ae7b41e614be093d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Menyimpan Embeddings yang dihasilkan SBERT \n",
    "train_embeddings_sbert = sbert_embedding(train_data['essay'], tokenizer_sbert, sbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ee0096a-4465-4d39-8a9a-b5b052d59e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding essay embeddings:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8edf8abed44bcdacc1ef6a5929d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_embeddings_sbert = sbert_embedding(test_data['essay'], tokenizer_sbert, sbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98fb41cf-ddc7-4b7a-afd0-ab5428fe29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349, 2304)\n"
     ]
    }
   ],
   "source": [
    "print(train_embeddings_sbert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ecf55bf4-d57f-43ad-9aed-a788bfcfabce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 2304)\n"
     ]
    }
   ],
   "source": [
    "print(test_embeddings_sbert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "479d4f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04381351, -0.2929353 , -0.19225127, ...,  0.23661417,\n",
       "        -0.10249371, -0.28071922],\n",
       "       [-0.02088969, -0.41765168, -0.22702567, ..., -0.06197346,\n",
       "        -0.22347306, -0.34798333],\n",
       "       [-0.11438533, -0.32085142, -0.21381213, ..., -0.07138707,\n",
       "        -0.14463581, -0.3485329 ],\n",
       "       ...,\n",
       "       [ 0.10865152, -0.41539565, -0.21752897, ...,  0.1955385 ,\n",
       "         0.39938778, -0.30938926],\n",
       "       [ 0.04142565, -0.26300216, -0.20707145, ...,  0.08358064,\n",
       "         0.11153191, -0.28362325],\n",
       "       [ 0.10163353, -0.3595566 , -0.1719999 , ...,  0.10730707,\n",
       "         0.2526765 , -0.22698738]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.shape(train_embeddings_sbert)\n",
    "train_embeddings_sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b60586eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0415417 , -0.39821178, -0.19009815, ...,  0.08180489,\n",
       "        -0.16593353, -0.2926095 ],\n",
       "       [-0.05087076, -0.18135877, -0.17721929, ..., -0.1233057 ,\n",
       "        -0.04336806, -0.24059765],\n",
       "       [-0.00315987, -0.3030437 , -0.21211623, ..., -0.05258212,\n",
       "         0.03890963, -0.17849146],\n",
       "       ...,\n",
       "       [ 0.11772002, -0.3721691 , -0.20598808, ..., -0.03406287,\n",
       "         0.13292538, -0.44794503],\n",
       "       [ 0.07173629, -0.34980538, -0.19100817, ...,  0.262721  ,\n",
       "         0.19996798, -0.20783165],\n",
       "       [ 0.14203782, -0.3976082 , -0.18679394, ...,  0.2678221 ,\n",
       "         0.32901722, -0.25610444]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings_sbert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29094334",
   "metadata": {},
   "source": [
    "# REGRESI FCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f995c30",
   "metadata": {},
   "source": [
    "INISIALISASI FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "faf3cd82-b670-4994-83b1-08c79be09469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menginisialisasi FCNN\n",
    "class FCNN(nn.Module):\n",
    "    # Fungsi untuk menentukan pengaturan layer\n",
    "    def __init__(self, input_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 397)  # Layer pertama: input_size -> 256\n",
    "        self.dropout1 = nn.Dropout(0.3) \n",
    "        self.fc2 = nn.Linear(397, 32)           # Layer kedua: 256 -> 96\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(32, 1)              # Layer ketiga: 96 -> 1\n",
    "        self.sigmoid = nn.Sigmoid()             # Fungsi aktivasi Sigmoid\n",
    "    \n",
    "    # Fungsi untuk untuk melakukan feedforward\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))              # Aktivasi ReLU di layer pertama\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))              # Aktivasi ReLU di layer kedua\n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc3(x)                                     # Layer ketiga (output layer)\n",
    "        return self.sigmoid(x)                        # Output dengan fungsi aktivasi Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11c3e1",
   "metadata": {},
   "source": [
    "TESTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1dc4d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(trained_model, cost_function, test_loader):\n",
    "    trained_model.eval() # Mengatur model ke mode evaluasi (eval mode)\n",
    "    test_loss = 0.\n",
    "    samples = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, targets) in enumerate(test_loader):\n",
    "            \n",
    "            # Menghapus dimensi yang tidak perlu dari inputs dan mentransfer ke device\n",
    "            inputs = inputs.squeeze(dim=1).to(device)\n",
    "            \n",
    "            # Menyesuaikan dimensi targets dan mentransfer ke device\n",
    "            targets = targets.reshape(targets.shape[0], 1).to(device)\n",
    "            \n",
    "            # Menghitung output model (prediksi) dari inputs\n",
    "            outputs = trained_model(inputs).reshape(-1, 1)\n",
    "            \n",
    "            # Menghitung nilai loss dengan membandingkan outputs dengan targets\n",
    "            loss = cost_function(outputs, targets)\n",
    "            \n",
    "            # Menghitung jumlah sampel dalam batch\n",
    "            samples += inputs.shape[0]\n",
    "            \n",
    "            # Menambahkan nilai loss dari batch ke test_loss\n",
    "            test_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "    # Menghitung rata-rata loss di seluruh batch (samples)\n",
    "    avg_loss = test_loss / samples\n",
    "    \n",
    "    # Mengembalikan nilai rata-rata loss\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72e3c7",
   "metadata": {},
   "source": [
    "TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "105c5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh fungsi untuk training model\n",
    "def training_step(model, cost_function, train_loader, test_loader, save_path, num_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Mengatur model ke mode pelatihan\n",
    "        \n",
    "        # Mengatur gradien parameter ke nilai nol untuk iterasi\n",
    "        running_loss = 0.\n",
    "        samples = 0.\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            # Menghapus dimensi yang tidak perlu dari inputs dan mentransfer ke device\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "            # Menyesuaikan dimensi targets dan mentransfer ke device\n",
    "            targets = targets.reshape(targets.shape[0], 1).to(device)\n",
    "            \n",
    "            # Menghitung output model (prediksi) dari inputs\n",
    "            outputs = model(inputs).reshape(-1, 1)\n",
    "            \n",
    "            # Menghitung nilai loss dengan membandingkan outputs dengan targets\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Mengatur gradien parameter ke nilai nol untuk iterasi berikutnya\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Melakukan backpropagation untuk menghitung gradien loss terhadap parameter model\n",
    "            loss.backward()\n",
    "            \n",
    "            # Melakukan optimizer untuk mengupdate parameter model berdasarkan gradien\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Menambahkan nilai loss dari batch ke running_loss\n",
    "            running_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "            # Menghitung jumlah sampel dalam batch\n",
    "            samples += inputs.shape[0]\n",
    "        \n",
    "        # Menghitung rata-rata loss pada data latih\n",
    "        train_loss = running_loss / samples\n",
    "        \n",
    "         # Evaluasi pada data uji\n",
    "        test_loss = test_step(model, cost_function, test_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print('Epoch: {:}/{:}\\tLoss/train: {:.5f}\\tLoss/test: {:.5f}'.format(epoch+1, num_epochs, train_loss, test_loss))\n",
    "    \n",
    "    # Simpan model setelah pelatihan\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved at {save_path}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44aae7",
   "metadata": {},
   "source": [
    "SCORING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b02d6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi untuk melakukan prediksi pada data uji\n",
    "def scoring(trained_model, test_loader):\n",
    "    trained_model.to(device)  # Move the model to the correct device\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # Lakukan prediksi dengan model yang telah dilatih\n",
    "            outputs = trained_model(inputs)\n",
    "            \n",
    "            # Menyimpan prediksi (outputs) dalam bentuk list predictions\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf218006",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "36da919b-309a-4e92-ae31-e89cfbe0cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e9831-1d61-4cd1-a27d-1092768abd13",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a8c309f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "\t\t\tTraining model SBERT: \n",
      "------------------------------------------------------------------\n",
      "Epoch: 1/100\tLoss/train: 0.01860\tLoss/test: 0.01840\n",
      "Epoch: 2/100\tLoss/train: 0.01590\tLoss/test: 0.01690\n",
      "Epoch: 3/100\tLoss/train: 0.01427\tLoss/test: 0.01592\n",
      "Epoch: 4/100\tLoss/train: 0.01316\tLoss/test: 0.01518\n",
      "Epoch: 5/100\tLoss/train: 0.01189\tLoss/test: 0.01452\n",
      "Epoch: 6/100\tLoss/train: 0.01161\tLoss/test: 0.01362\n",
      "Epoch: 7/100\tLoss/train: 0.01139\tLoss/test: 0.01297\n",
      "Epoch: 8/100\tLoss/train: 0.01073\tLoss/test: 0.01259\n",
      "Epoch: 9/100\tLoss/train: 0.01031\tLoss/test: 0.01237\n",
      "Epoch: 10/100\tLoss/train: 0.00982\tLoss/test: 0.01212\n",
      "Epoch: 11/100\tLoss/train: 0.00999\tLoss/test: 0.01186\n",
      "Epoch: 12/100\tLoss/train: 0.00941\tLoss/test: 0.01187\n",
      "Epoch: 13/100\tLoss/train: 0.00938\tLoss/test: 0.01177\n",
      "Epoch: 14/100\tLoss/train: 0.00966\tLoss/test: 0.01180\n",
      "Epoch: 15/100\tLoss/train: 0.00970\tLoss/test: 0.01163\n",
      "Epoch: 16/100\tLoss/train: 0.00932\tLoss/test: 0.01154\n",
      "Epoch: 17/100\tLoss/train: 0.00930\tLoss/test: 0.01157\n",
      "Epoch: 18/100\tLoss/train: 0.00893\tLoss/test: 0.01136\n",
      "Epoch: 19/100\tLoss/train: 0.00887\tLoss/test: 0.01137\n",
      "Epoch: 20/100\tLoss/train: 0.00943\tLoss/test: 0.01151\n",
      "Epoch: 21/100\tLoss/train: 0.00923\tLoss/test: 0.01131\n",
      "Epoch: 22/100\tLoss/train: 0.00869\tLoss/test: 0.01135\n",
      "Epoch: 23/100\tLoss/train: 0.00841\tLoss/test: 0.01122\n",
      "Epoch: 24/100\tLoss/train: 0.00853\tLoss/test: 0.01128\n",
      "Epoch: 25/100\tLoss/train: 0.00879\tLoss/test: 0.01117\n",
      "Epoch: 26/100\tLoss/train: 0.00891\tLoss/test: 0.01097\n",
      "Epoch: 27/100\tLoss/train: 0.00881\tLoss/test: 0.01105\n",
      "Epoch: 28/100\tLoss/train: 0.00855\tLoss/test: 0.01122\n",
      "Epoch: 29/100\tLoss/train: 0.00849\tLoss/test: 0.01098\n",
      "Epoch: 30/100\tLoss/train: 0.00838\tLoss/test: 0.01100\n",
      "Epoch: 31/100\tLoss/train: 0.00834\tLoss/test: 0.01095\n",
      "Epoch: 32/100\tLoss/train: 0.00767\tLoss/test: 0.01098\n",
      "Epoch: 33/100\tLoss/train: 0.00818\tLoss/test: 0.01085\n",
      "Epoch: 34/100\tLoss/train: 0.00794\tLoss/test: 0.01085\n",
      "Epoch: 35/100\tLoss/train: 0.00857\tLoss/test: 0.01081\n",
      "Epoch: 36/100\tLoss/train: 0.00817\tLoss/test: 0.01098\n",
      "Epoch: 37/100\tLoss/train: 0.00763\tLoss/test: 0.01080\n",
      "Epoch: 38/100\tLoss/train: 0.00710\tLoss/test: 0.01076\n",
      "Epoch: 39/100\tLoss/train: 0.00763\tLoss/test: 0.01075\n",
      "Epoch: 40/100\tLoss/train: 0.00777\tLoss/test: 0.01072\n",
      "Epoch: 41/100\tLoss/train: 0.00755\tLoss/test: 0.01078\n",
      "Epoch: 42/100\tLoss/train: 0.00771\tLoss/test: 0.01074\n",
      "Epoch: 43/100\tLoss/train: 0.00746\tLoss/test: 0.01063\n",
      "Epoch: 44/100\tLoss/train: 0.00751\tLoss/test: 0.01074\n",
      "Epoch: 45/100\tLoss/train: 0.00754\tLoss/test: 0.01054\n",
      "Epoch: 46/100\tLoss/train: 0.00737\tLoss/test: 0.01060\n",
      "Epoch: 47/100\tLoss/train: 0.00744\tLoss/test: 0.01060\n",
      "Epoch: 48/100\tLoss/train: 0.00770\tLoss/test: 0.01080\n",
      "Epoch: 49/100\tLoss/train: 0.00763\tLoss/test: 0.01045\n",
      "Epoch: 50/100\tLoss/train: 0.00750\tLoss/test: 0.01056\n",
      "Epoch: 51/100\tLoss/train: 0.00719\tLoss/test: 0.01083\n",
      "Epoch: 52/100\tLoss/train: 0.00707\tLoss/test: 0.01044\n",
      "Epoch: 53/100\tLoss/train: 0.00752\tLoss/test: 0.01036\n",
      "Epoch: 54/100\tLoss/train: 0.00671\tLoss/test: 0.01066\n",
      "Epoch: 55/100\tLoss/train: 0.00684\tLoss/test: 0.01041\n",
      "Epoch: 56/100\tLoss/train: 0.00669\tLoss/test: 0.01053\n",
      "Epoch: 57/100\tLoss/train: 0.00714\tLoss/test: 0.01042\n",
      "Epoch: 58/100\tLoss/train: 0.00701\tLoss/test: 0.01020\n",
      "Epoch: 59/100\tLoss/train: 0.00681\tLoss/test: 0.01044\n",
      "Epoch: 60/100\tLoss/train: 0.00668\tLoss/test: 0.01030\n",
      "Epoch: 61/100\tLoss/train: 0.00668\tLoss/test: 0.01037\n",
      "Epoch: 62/100\tLoss/train: 0.00667\tLoss/test: 0.01024\n",
      "Epoch: 63/100\tLoss/train: 0.00692\tLoss/test: 0.01031\n",
      "Epoch: 64/100\tLoss/train: 0.00690\tLoss/test: 0.01020\n",
      "Epoch: 65/100\tLoss/train: 0.00653\tLoss/test: 0.01028\n",
      "Epoch: 66/100\tLoss/train: 0.00671\tLoss/test: 0.01013\n",
      "Epoch: 67/100\tLoss/train: 0.00636\tLoss/test: 0.01032\n",
      "Epoch: 68/100\tLoss/train: 0.00646\tLoss/test: 0.01025\n",
      "Epoch: 69/100\tLoss/train: 0.00666\tLoss/test: 0.01026\n",
      "Epoch: 70/100\tLoss/train: 0.00636\tLoss/test: 0.01014\n",
      "Epoch: 71/100\tLoss/train: 0.00636\tLoss/test: 0.01019\n",
      "Epoch: 72/100\tLoss/train: 0.00625\tLoss/test: 0.01018\n",
      "Epoch: 73/100\tLoss/train: 0.00620\tLoss/test: 0.01018\n",
      "Epoch: 74/100\tLoss/train: 0.00617\tLoss/test: 0.01005\n",
      "Epoch: 75/100\tLoss/train: 0.00655\tLoss/test: 0.01031\n",
      "Epoch: 76/100\tLoss/train: 0.00616\tLoss/test: 0.01014\n",
      "Epoch: 77/100\tLoss/train: 0.00660\tLoss/test: 0.00999\n",
      "Epoch: 78/100\tLoss/train: 0.00583\tLoss/test: 0.01031\n",
      "Epoch: 79/100\tLoss/train: 0.00589\tLoss/test: 0.01005\n",
      "Epoch: 80/100\tLoss/train: 0.00608\tLoss/test: 0.01027\n",
      "Epoch: 81/100\tLoss/train: 0.00587\tLoss/test: 0.01025\n",
      "Epoch: 82/100\tLoss/train: 0.00586\tLoss/test: 0.01019\n",
      "Epoch: 83/100\tLoss/train: 0.00593\tLoss/test: 0.01012\n",
      "Epoch: 84/100\tLoss/train: 0.00581\tLoss/test: 0.01016\n",
      "Epoch: 85/100\tLoss/train: 0.00600\tLoss/test: 0.01015\n",
      "Epoch: 86/100\tLoss/train: 0.00577\tLoss/test: 0.01034\n",
      "Epoch: 87/100\tLoss/train: 0.00600\tLoss/test: 0.00997\n",
      "Epoch: 88/100\tLoss/train: 0.00538\tLoss/test: 0.01023\n",
      "Epoch: 89/100\tLoss/train: 0.00597\tLoss/test: 0.01009\n",
      "Epoch: 90/100\tLoss/train: 0.00572\tLoss/test: 0.01004\n",
      "Epoch: 91/100\tLoss/train: 0.00572\tLoss/test: 0.00999\n",
      "Epoch: 92/100\tLoss/train: 0.00570\tLoss/test: 0.01009\n",
      "Epoch: 93/100\tLoss/train: 0.00585\tLoss/test: 0.01003\n",
      "Epoch: 94/100\tLoss/train: 0.00539\tLoss/test: 0.01006\n",
      "Epoch: 95/100\tLoss/train: 0.00557\tLoss/test: 0.01006\n",
      "Epoch: 96/100\tLoss/train: 0.00543\tLoss/test: 0.01006\n",
      "Epoch: 97/100\tLoss/train: 0.00544\tLoss/test: 0.01010\n",
      "Epoch: 98/100\tLoss/train: 0.00582\tLoss/test: 0.01003\n",
      "Epoch: 99/100\tLoss/train: 0.00515\tLoss/test: 0.01020\n",
      "Epoch: 100/100\tLoss/train: 0.00569\tLoss/test: 0.01023\n",
      "Model saved at model_sbert_v7-mean_max_cls_rata2_banyak.pth\n",
      "Training time: 33.40833830833435\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "input_size = 2304\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "epochs = 100\n",
    "\n",
    "# TRAINING\n",
    "# Inisialisasi model, loader, dan fungsi loss\n",
    "model_sbert = FCNN(input_size).to(device)  # Ganti dengan model Anda\n",
    "cost_function = torch.nn.MSELoss()\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_sbert = get_loader(train_data, id2emb_train, train_embeddings_sbert, batch_size, shuffle=True)\n",
    "test_loader_sbert = get_loader(test_data, id2emb_test, test_embeddings_sbert, batch_size, shuffle=False)\n",
    "\n",
    "print('------------------------------------------------------------------')\n",
    "print(f\"\\t\\t\\tTraining model SBERT: \")\n",
    "print('------------------------------------------------------------------')\n",
    "# Path tempat model akan disimpan dan dimuat\n",
    "save_path = 'model_sbert_v7-mean_max_cls_rata2_banyak.pth'\n",
    "\n",
    "start_time_sbert = time.time()\n",
    "train_loss_sbert, test_loss_sbert = training_step(model_sbert, cost_function, train_loader_sbert, test_loader_sbert, save_path, epochs, lr)\n",
    "end_time_sbert = time.time()\n",
    "\n",
    "print('Training time:', end_time_sbert - start_time_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d2851-6ab7-43a9-9129-b6088f631130",
   "metadata": {},
   "source": [
    "SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "608de217-f7e9-444e-a014-ed3a94ba22a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "\t\t\tScoring Essay: \n",
      "------------------------------------------------------------------\n",
      "Evaluation time: 0.1551046371459961\n"
     ]
    }
   ],
   "source": [
    "# Memuat model yang telah dilatih\n",
    "model_sbert_trained = model_sbert \n",
    "model_sbert_trained.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Menggunakan model untuk prediksi pada data uji\n",
    "print('------------------------------------------------------------------')\n",
    "print(f\"\\t\\t\\tScoring Essay: \")\n",
    "print('------------------------------------------------------------------')\n",
    "start_time_eval = time.time()\n",
    "test_predictions_sbert = scoring(model_sbert_trained, test_loader_sbert)\n",
    "end_time_eval = time.time()\n",
    "print('Evaluation time:', end_time_eval - start_time_eval)\n",
    "\n",
    "# store train_df, test_df and predictions\n",
    "train_df_sbert = train_data\n",
    "test_df_sbert = test_data\n",
    "preds_sbert = test_predictions_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa091b",
   "metadata": {},
   "source": [
    "# RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4b5f98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_df(test_df, model_preds):\n",
    "\n",
    "  # create new results df with model scaled preds\n",
    "  preds_df = pd.DataFrame(model_preds)\n",
    "  results_df = test_df.reset_index(drop=True)\\\n",
    "              .join(preds_df)\\\n",
    "              .rename(columns={0:'prediction'})\\\n",
    "              .sort_values(by='essay_id')\\\n",
    "              .reset_index(drop=True)\n",
    "\n",
    "  # move score to last colum\n",
    "  s_df = results_df.pop('score')\n",
    "  results_df['score'] = s_df\n",
    "\n",
    "  return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "87715126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...</td>\n",
       "      <td>0.725677</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers have caused many people money, frien...</td>\n",
       "      <td>0.663973</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1: The effects that computers have o...</td>\n",
       "      <td>0.642397</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you think computers are helpful? Well you s...</td>\n",
       "      <td>0.634302</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>As more and more people are becoming accustome...</td>\n",
       "      <td>0.697494</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>21354</td>\n",
       "      <td>8</td>\n",
       "      <td>i woke up at @NUM1 from a cal from my cousin ...</td>\n",
       "      <td>0.604292</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>21369</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is the key I think that being happy ...</td>\n",
       "      <td>0.544167</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>21407</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...</td>\n",
       "      <td>0.608010</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>21509</td>\n",
       "      <td>8</td>\n",
       "      <td>So one day I was at home babysitting my little...</td>\n",
       "      <td>0.602508</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>21596</td>\n",
       "      <td>8</td>\n",
       "      <td>I woke up just like any other day happy yet l...</td>\n",
       "      <td>0.612182</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    essay_id essay_set                                              essay  \\\n",
       "0         73         1  Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...   \n",
       "1         79         1  Computers have caused many people money, frien...   \n",
       "2        120         1  Dear @CAPS1: The effects that computers have o...   \n",
       "3        121         1  Do you think computers are helpful? Well you s...   \n",
       "4        142         1  As more and more people are becoming accustome...   \n",
       "..       ...       ...                                                ...   \n",
       "146    21354         8   i woke up at @NUM1 from a cal from my cousin ...   \n",
       "147    21369         8   Laughter is the key I think that being happy ...   \n",
       "148    21407         8  The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...   \n",
       "149    21509         8  So one day I was at home babysitting my little...   \n",
       "150    21596         8   I woke up just like any other day happy yet l...   \n",
       "\n",
       "     prediction     score  \n",
       "0      0.725677  0.600000  \n",
       "1      0.663973  0.800000  \n",
       "2      0.642397  0.600000  \n",
       "3      0.634302  0.600000  \n",
       "4      0.697494  0.800000  \n",
       "..          ...       ...  \n",
       "146    0.604292  0.583333  \n",
       "147    0.544167  0.483333  \n",
       "148    0.608010  0.666667  \n",
       "149    0.602508  0.500000  \n",
       "150    0.612182  0.516667  \n",
       "\n",
       "[151 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_df(test_df_sbert, preds_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce84069",
   "metadata": {},
   "source": [
    "# DENORMALISASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "78d1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize_score(score, min_max_range):\n",
    "    # Mendapatkan nilai minimum dan maksimum dari rentang normalisasi\n",
    "    min_score, max_score = min_max_range\n",
    "    \n",
    "    # Mengembalikan skor esai yang sudah dinormalisasi ke rentang aslinya\n",
    "    return round(score * (max_score - min_score) + min_score)\n",
    "\n",
    "def restore_original_scores(df, preds, min_max_ranges):\n",
    "    # Membuat salinan dataframe untuk menghindari modifikasi dataframe asli\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Mendapatkan kolom skor aktual\n",
    "    actual_scores = df_copy['score'].values\n",
    "    \n",
    "    # Mendapatkan kolom essay_set\n",
    "    essay_sets = df_copy['essay_set'].values\n",
    "    \n",
    "    # Memastikan preds memiliki panjang yang sama dengan jumlah data\n",
    "    assert len(preds) == len(df_copy), \"Length of predictions does not match length of dataframe\"\n",
    "    \n",
    "    # Memulihkan skor prediksi dan skor aktual ke rentang aslinya\n",
    "    restored_preds = [inverse_normalize_score(pred, min_max_ranges[essay_set]) for pred, essay_set in zip(preds, essay_sets)]\n",
    "    restored_actuals = [inverse_normalize_score(actual, min_max_ranges[essay_set]) for actual, essay_set in zip(actual_scores, essay_sets)]\n",
    "    \n",
    "    # Mengganti kolom skor prediksi dan aktual dengan skor yang sudah dipulihkan\n",
    "    df_copy['prediction'] = restored_preds\n",
    "    df_copy['score'] = restored_actuals\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "72dd7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 Newspaper, @CAPS2 though computers...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1614</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 I think that computers are benefic...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1392</td>\n",
       "      <td>1</td>\n",
       "      <td>I think that computers have a good effect on p...</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1055</td>\n",
       "      <td>1</td>\n",
       "      <td>To whom it @MONTH1 concern, To many people it ...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>21090</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of Laughter @CAPS2 friends and I, a...</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>21369</td>\n",
       "      <td>8</td>\n",
       "      <td>Laughter is the key I think that being happy ...</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>21407</td>\n",
       "      <td>8</td>\n",
       "      <td>The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>21204</td>\n",
       "      <td>8</td>\n",
       "      <td>It was the first day of sophomore year. I had...</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>20747</td>\n",
       "      <td>8</td>\n",
       "      <td>These days you don't really need a reason to ...</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    essay_id essay_set                                              essay  \\\n",
       "0        560         1  Dear @CAPS1 Newspaper, @CAPS2 though computers...   \n",
       "1       1614         1  Dear @CAPS1 I think that computers are benefic...   \n",
       "2       1392         1  I think that computers have a good effect on p...   \n",
       "3         73         1  Dear The @ORGANIZATION1, \"@CAPS1, @CAPS1, @CAP...   \n",
       "4       1055         1  To whom it @MONTH1 concern, To many people it ...   \n",
       "..       ...       ...                                                ...   \n",
       "146    21090         8  The @CAPS1 of Laughter @CAPS2 friends and I, a...   \n",
       "147    21369         8   Laughter is the key I think that being happy ...   \n",
       "148    21407         8  The @CAPS1 of @CAPS2 @CAPS3 I think of laughte...   \n",
       "149    21204         8   It was the first day of sophomore year. I had...   \n",
       "150    20747         8   These days you don't really need a reason to ...   \n",
       "\n",
       "     score  prediction  \n",
       "0        8           9  \n",
       "1        9           8  \n",
       "2        5           9  \n",
       "3        8           9  \n",
       "4        8           8  \n",
       "..     ...         ...  \n",
       "146     37          41  \n",
       "147     29          33  \n",
       "148     40          36  \n",
       "149     31          41  \n",
       "150     38          37  \n",
       "\n",
       "[151 rows x 5 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengembalikan skor prediksi dan skor aktual ke rentang awalnya\n",
    "restored_results_df_sbert = restore_original_scores(test_df_sbert, preds_sbert, min_max_ranges)\n",
    "\n",
    "# Cetak hasilnya\n",
    "print(\"Restored Results:\")\n",
    "restored_results_df_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144dadc",
   "metadata": {},
   "source": [
    "# EVALUASI QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0640c52c-d4a5-4cd8-9f09-1231b01e6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_qwk(actuals, preds):\n",
    "    # Menentukan nilai minimum dan maksimum untuk rentang skor\n",
    "    min_rating = min(min(actuals), min(preds))\n",
    "    max_rating = max(max(actuals), max(preds))\n",
    "    \n",
    "    # Jumlah total kemungkinan penilaian\n",
    "    num_ratings = max_rating - min_rating + 1\n",
    "\n",
    "    # Membuat matriks bobot W\n",
    "    weight_mat = np.zeros((num_ratings, num_ratings))\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            weight_mat[i][j] = ((i - j) ** 2) / ((num_ratings - 1) ** 2)\n",
    "\n",
    "    # Membuat matriks observasi O\n",
    "    conf_mat = np.zeros((num_ratings, num_ratings))\n",
    "    for actual, pred in zip(actuals, preds):\n",
    "        conf_mat[actual - min_rating][pred - min_rating] += 1\n",
    "\n",
    "    # Membuat matriks ekspektasi E\n",
    "    actual_hist = np.zeros(num_ratings)\n",
    "    pred_hist = np.zeros(num_ratings)\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            actual_hist[i] += conf_mat[i][j]\n",
    "            pred_hist[j] += conf_mat[i][j]\n",
    "\n",
    "    expected_mat = np.outer(actual_hist, pred_hist) / len(actuals)\n",
    "\n",
    "    # Menghitung nilai QWK\n",
    "    num_agreements = np.sum(weight_mat * conf_mat)\n",
    "    num_possible_agreements = np.sum(weight_mat * expected_mat)\n",
    "    kappa_score = 1 - (num_agreements / num_possible_agreements)\n",
    "\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3443356b-4b36-4618-b361-924b045088a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung QWK per set\n",
    "def calculate_qwk_per_set(df):\n",
    "    # Menyimpan nilai QWK per set dalam dictionary\n",
    "    qwk_per_set = {}\n",
    "    \n",
    "    # Mendapatkan unique essay_set values\n",
    "    essay_sets = df['essay_set'].unique()\n",
    "    \n",
    "    # Iterasi melalui setiap essay_set\n",
    "    for essay_set in essay_sets:\n",
    "        # Filter dataframe berdasarkan essay_set\n",
    "        subset_df = df[df['essay_set'] == essay_set]\n",
    "        \n",
    "        # Mengekstrak skor aktual dan prediksi dari subset dataframe\n",
    "        actual_scores = subset_df['score'].astype(int)\n",
    "        predicted_scores = subset_df['prediction'].astype(int)\n",
    "        \n",
    "        # Menghitung QWK untuk subset tersebut\n",
    "        qwk = calculate_qwk(actual_scores, predicted_scores)\n",
    "        \n",
    "        # Menyimpan nilai QWK ke dalam dictionary\n",
    "        qwk_per_set[f'Set {essay_set}'] = qwk\n",
    "    \n",
    "    return qwk_per_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "695d8cda-6896-4180-a9a3-fa9fc45647ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Weighted Kappa Score for Set 1: 0.2589321557607386\n",
      "Quadratic Weighted Kappa Score for Set 2: 0.1071428571428572\n",
      "Quadratic Weighted Kappa Score for Set 7: 0.5467625899280576\n",
      "Quadratic Weighted Kappa Score for Set 8: 0.1243561442236939\n",
      "Average Quadratic Weighted Kappa Score: 0.2592984367638368\n"
     ]
    }
   ],
   "source": [
    "# Menghitung QWK per set\n",
    "qwk_per_set_sbert = calculate_qwk_per_set(restored_results_df_sbert)\n",
    "\n",
    "# Menampilkan nilai QWK per set\n",
    "for essay_set, qwk_score in qwk_per_set_sbert.items():\n",
    "    print(f\"Quadratic Weighted Kappa Score for {essay_set}: {qwk_score}\")\n",
    "\n",
    "# Menghitung rata-rata nilai QWK dari semua set\n",
    "average_qwk_sbert = np.mean(list(qwk_per_set_sbert.values()))\n",
    "\n",
    "# Menampilkan rata-rata nilai QWK\n",
    "print(\"Average Quadratic Weighted Kappa Score:\", average_qwk_sbert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
